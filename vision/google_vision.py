"""
Google Cloud Vision APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì—ì„œ ì •ë³´ë¥¼ ì¸ì‹í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
- ì´ë¯¸ì§€ ë¶„ì„ (ë¼ë²¨, í…ìŠ¤íŠ¸, ì–¼êµ´, ëœë“œë§ˆí¬ ë“±)
- OCR (í…ìŠ¤íŠ¸ ì¶”ì¶œ)
- ê°ì²´ ê°ì§€
- ì•ˆì „ í•„í„°ë§
"""

import os
from dotenv import load_dotenv
from google.cloud import vision
from google.cloud.vision_v1 import types

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class GoogleVisionAnalyzer:
    def __init__(self):
        """Google Cloud Vision í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        # ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼ ê²½ë¡œ í™•ì¸
        credentials_path = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
        
        if credentials_path:
            # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜ (í˜„ì¬ íŒŒì¼ ê¸°ì¤€ì´ ì•„ë‹Œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€)
            if not os.path.isabs(credentials_path):
                # í˜„ì¬ íŒŒì¼ì˜ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì°¾ê¸° (vision í´ë”ì˜ ìƒìœ„)
                current_dir = os.path.dirname(os.path.abspath(__file__))
                root_dir = os.path.dirname(current_dir)  # visionì˜ ìƒìœ„ = í”„ë¡œì íŠ¸ ë£¨íŠ¸
                credentials_path = os.path.join(root_dir, credentials_path)
            
            # ê²½ë¡œ ì •ê·œí™” (ë°±ìŠ¬ë˜ì‹œ/ìŠ¬ë˜ì‹œ í†µì¼)
            credentials_path = os.path.normpath(credentials_path)
            os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = credentials_path
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not os.path.exists(credentials_path):
                raise FileNotFoundError(
                    f"ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {credentials_path}\n"
                    f"íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\n"
                    f"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}"
                )
        
        try:
            self.client = vision.ImageAnnotatorClient()
            print(f"âœ… Google Cloud Vision í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ (ì¸ì¦ íŒŒì¼: {credentials_path})")
        except Exception as e:
            raise ValueError(
                f"Google Cloud Vision í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}\n"
                "ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼ì„ ì„¤ì •í•´ì£¼ì„¸ìš”:\n"
                "1. .env íŒŒì¼ì— GOOGLE_APPLICATION_CREDENTIALS=íŒŒì¼ëª….json ì„¤ì •\n"
                "2. ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •"
            )
    
    def load_image(self, image_path):
        """ì´ë¯¸ì§€ íŒŒì¼ì„ ì½ì–´ì„œ Vision API í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        with open(image_path, 'rb') as image_file:
            content = image_file.read()
        return types.Image(content=content)
    
    def analyze_image(self, image_path):
        """
        ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ë¼ë²¨, í…ìŠ¤íŠ¸, ì–¼êµ´ ë“±ì„ ì¶”ì¶œ
        
        Args:
            image_path: ë¶„ì„í•  ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print(f"\nğŸ“¸ ì´ë¯¸ì§€ ë¶„ì„ ì¤‘: {image_path}")
        
        image = self.load_image(image_path)
        
        # ë‹¤ì–‘í•œ ê¸°ëŠ¥ ìˆ˜í–‰
        from google.cloud.vision_v1 import types as vision_types
        
        features = [
            {'type_': vision_types.Feature.Type.LABEL_DETECTION},
            {'type_': vision_types.Feature.Type.TEXT_DETECTION},
            {'type_': vision_types.Feature.Type.FACE_DETECTION},
            {'type_': vision_types.Feature.Type.LANDMARK_DETECTION},
            {'type_': vision_types.Feature.Type.LOGO_DETECTION},
            {'type_': vision_types.Feature.Type.OBJECT_LOCALIZATION},
            {'type_': vision_types.Feature.Type.SAFE_SEARCH_DETECTION},
        ]
        
        response = self.client.annotate_image({
            'image': image,
            'features': features
        })
        
        result = {
            'labels': [(label.description, label.score) for label in response.label_annotations],
            'text': response.text_annotations[0].description if response.text_annotations else "",
            'faces': len(response.face_annotations),
            'landmarks': [landmark.description for landmark in response.landmark_annotations],
            'logos': [logo.description for logo in response.logo_annotations],
            'objects': [(obj.name, obj.score) for obj in response.localized_object_annotations],
            'safe_search': {
                'adult': response.safe_search_annotation.adult.name,
                'violence': response.safe_search_annotation.violence.name,
                'racy': response.safe_search_annotation.racy.name,
            }
        }
        
        return result
    
    def extract_text(self, image_path):
        """
        ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (OCR)
        
        Args:
            image_path: í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë¬¸ìì—´
        """
        print(f"\nğŸ“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘: {image_path}")
        
        image = self.load_image(image_path)
        response = self.client.text_detection(image=image)
        
        if response.text_annotations:
            return response.text_annotations[0].description
        return ""
    
    def detect_objects(self, image_path):
        """
        ì´ë¯¸ì§€ì—ì„œ ê°ì²´ ê°ì§€
        
        Args:
            image_path: ë¶„ì„í•  ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ê°ì§€ëœ ê°ì²´ ë¦¬ìŠ¤íŠ¸
        """
        print(f"\nğŸ” ê°ì²´ ê°ì§€ ì¤‘: {image_path}")
        
        image = self.load_image(image_path)
        response = self.client.object_localization(image=image)
        
        objects = []
        for obj in response.localized_object_annotations:
            # ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´ ì¶”ì¶œ
            vertices = []
            for vertex in obj.bounding_poly.normalized_vertices:
                vertices.append({
                    'x': vertex.x,
                    'y': vertex.y
                })
            
            objects.append({
                'name': obj.name,
                'score': obj.score,
                'vertices': vertices
            })
        
        return objects
    
    def detect_faces(self, image_path):
        """
        ì´ë¯¸ì§€ì—ì„œ ì–¼êµ´ ê°ì§€
        
        Args:
            image_path: ë¶„ì„í•  ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ê°ì§€ëœ ì–¼êµ´ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        print(f"\nğŸ˜Š ì–¼êµ´ ê°ì§€ ì¤‘: {image_path}")
        
        image = self.load_image(image_path)
        response = self.client.face_detection(image=image)
        
        faces = []
        for face in response.face_annotations:
            faces.append({
                'joy_likelihood': face.joy_likelihood.name,
                'sorrow_likelihood': face.sorrow_likelihood.name,
                'anger_likelihood': face.anger_likelihood.name,
                'surprise_likelihood': face.surprise_likelihood.name,
                'detection_confidence': face.detection_confidence,
            })
        
        return faces
    
    def print_analysis_results(self, results):
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
        print("\n" + "="*50)
        print("ğŸ“Š ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼")
        print("="*50)
        
        if results['labels']:
            print("\nğŸ·ï¸  ë¼ë²¨:")
            for label, score in results['labels'][:10]:  # ìƒìœ„ 10ê°œë§Œ
                print(f"   â€¢ {label} ({score:.2%})")
        
        if results['text']:
            print(f"\nğŸ“ ê°ì§€ëœ í…ìŠ¤íŠ¸:")
            print(f"   {results['text'][:200]}...")  # ì²˜ìŒ 200ìë§Œ
        
        if results['faces'] > 0:
            print(f"\nğŸ˜Š ì–¼êµ´: {results['faces']}ê°œ ê°ì§€")
        
        if results['landmarks']:
            print(f"\nğŸ—ºï¸  ëœë“œë§ˆí¬: {', '.join(results['landmarks'])}")
        
        if results['logos']:
            print(f"\nğŸ¢ ë¡œê³ : {', '.join(results['logos'])}")
        
        if results['objects']:
            print(f"\nğŸ¯ ê°ì²´:")
            for obj_name, score in results['objects']:
                print(f"   â€¢ {obj_name} ({score:.2%})")
        
        print(f"\nğŸ›¡ï¸  ì•ˆì „ í•„í„°:")
        print(f"   ì„±ì¸ ì½˜í…ì¸ : {results['safe_search']['adult']}")
        print(f"   í­ë ¥: {results['safe_search']['violence']}")
        print(f"   ì„ ì •ì : {results['safe_search']['racy']}")
        
        print("="*50)
    
    def print_text_results(self, text):
        """ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
        print("\n" + "="*50)
        print("ğŸ“ ì¶”ì¶œëœ í…ìŠ¤íŠ¸")
        print("="*50)
        if text:
            print(text)
        else:
            print("í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("="*50)
    
    def print_objects_results(self, objects):
        """ê°ì§€ëœ ê°ì²´ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
        print("\n" + "="*50)
        print("ğŸ” ê°ì§€ëœ ê°ì²´")
        print("="*50)
        if objects:
            for obj in objects:
                print(f"\n  â€¢ {obj['name']}")
                print(f"    ì‹ ë¢°ë„: {obj['score']:.2%}")
        else:
            print("ê°ì§€ëœ ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("="*50)
    
    def print_faces_results(self, faces):
        """ê°ì§€ëœ ì–¼êµ´ì„ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
        print("\n" + "="*50)
        print("ğŸ˜Š ê°ì§€ëœ ì–¼êµ´")
        print("="*50)
        if faces:
            for i, face in enumerate(faces, 1):
                print(f"\n  ì–¼êµ´ {i}:")
                print(f"    ê¸°ì¨: {face['joy_likelihood']}")
                print(f"    ìŠ¬í””: {face['sorrow_likelihood']}")
                print(f"    ë¶„ë…¸: {face['anger_likelihood']}")
                print(f"    ë†€ëŒ: {face['surprise_likelihood']}")
                print(f"    ê°ì§€ ì‹ ë¢°ë„: {face['detection_confidence']:.2%}")
        else:
            print("ê°ì§€ëœ ì–¼êµ´ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("="*50)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        analyzer = GoogleVisionAnalyzer()
        
        # ì´ë¯¸ì§€ ê²½ë¡œ ì…ë ¥ ë°›ê¸°
        image_path = input("ë¶„ì„í•  ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        
        if not os.path.exists(image_path):
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
            return
        
        print("\nì–´ë–¤ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
        print("1. ì „ì²´ ë¶„ì„ (ë¼ë²¨, í…ìŠ¤íŠ¸, ê°ì²´, ì–¼êµ´ ë“±)")
        print("2. í…ìŠ¤íŠ¸ ì¶”ì¶œ (OCR)")
        print("3. ê°ì²´ ê°ì§€")
        print("4. ì–¼êµ´ ê°ì§€")
        print("5. ëª¨ë“  ë¶„ì„ ìˆ˜í–‰")
        
        choice = input("\nì„ íƒ (1-5): ").strip()
        
        if choice == "1":
            results = analyzer.analyze_image(image_path)
            analyzer.print_analysis_results(results)
        
        elif choice == "2":
            text = analyzer.extract_text(image_path)
            analyzer.print_text_results(text)
        
        elif choice == "3":
            objects = analyzer.detect_objects(image_path)
            analyzer.print_objects_results(objects)
        
        elif choice == "4":
            faces = analyzer.detect_faces(image_path)
            analyzer.print_faces_results(faces)
        
        elif choice == "5":
            # ì „ì²´ ë¶„ì„
            results = analyzer.analyze_image(image_path)
            analyzer.print_analysis_results(results)
            
            text = analyzer.extract_text(image_path)
            analyzer.print_text_results(text)
            
            objects = analyzer.detect_objects(image_path)
            analyzer.print_objects_results(objects)
            
            faces = analyzer.detect_faces(image_path)
            analyzer.print_faces_results(faces)
        
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
    
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

